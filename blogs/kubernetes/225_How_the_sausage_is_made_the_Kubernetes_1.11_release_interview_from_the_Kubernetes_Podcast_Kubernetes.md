|             Article Data             ||
| ----------------- | ----------------- |
| URL               | https://kubernetes.io/blog/2018/07/16/how-the-sausage-is-made-the-kubernetes-1.11-release-interview-from-the-kubernetes-podcast/        |
| Tags              | [kubernetes]       |
| Date Create       | 2018-07-16 00:00:00 &#43;0000 UTC |
| Date Parse        | 2021-12-06 10:51:20.7047611 &#43;0300 MSK m=&#43;2.293796701  |

# How the sausage is made: the Kubernetes 1.11 release interview, from the Kubernetes Podcast | Kubernetes

	
	
	
	
	: Craig Box (Google)
At KubeCon EU, my colleague Adam Glick and I were pleased to announce the [Kubernetes Podcast from Google](https://kubernetespodcast.com/). In this weekly conversation, we focus on all the great things that are happening in the world of Kubernetes and Cloud Native. From the news of the week, to interviews with people in the community, we help you stay up to date on everything Kubernetes.
We [recently had the pleasure of speaking](https://kubernetespodcast.com/episode/010-kubernetes-1.11/) to the release manager for Kubernetes 1.11, Josh Berkus from Red Hat, and the release manager for the upcoming 1.12, Tim Pepper from VMware.
In this conversation we learned about the release process, the impact of quarterly releases on end users, and how Kubernetes is like baking.
I encourage you to listen to [the podcast version](https://kubernetespodcast.com/episode/010-kubernetes-1.11/) if you have a commute, or a dog to walk. If you like what you hear, [we encourage you to subscribe](https://kubernetespodcast.com/subscribe)! In case you&#39;re short on time, or just want to browse quickly, we are delighted to share the transcript with you.

---

JOSH BERKUS: Well, thank you. Congratulations for me, because my job is done.
[LAUGHTER]
Congratulations and sympathy for Tim.
[LAUGH]
TIM PEPPER: Thank you, and I guess thank you?
[LAUGH]

JOSH BERKUS: We have a quarterly release cycle. So every three months, we&#39;re releasing. And ideally and fortunately, this is actually now how we are doing things. Somewhere around two, three weeks before the previous release, somebody volunteers to be the release lead. That person is confirmed by [SIG Release](https://github.com/kubernetes/sig-release). So far, we&#39;ve never had more than one volunteer, so there hasn&#39;t been really a fight about it.
And then that person starts working with others to put together [a team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/release_team.md) called the release team. Tim&#39;s just gone through this with Stephen Augustus and [picking out a whole bunch of people](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.12/release_team.md). And then after or a little before— probably after, because we want to wait for the retrospective from the previous release— the release lead then sets [a schedule](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/release-1.11.md) for the upcoming release, as in when all the deadlines will be.
And this is a thing, because we&#39;re still tinkering with relative deadlines, and how long should code freeze be, and how should we track features? Because we don&#39;t feel that we&#39;ve gotten down that sort of cadence perfectly yet. I mean, like, we&#39;ve done pretty well, but we don&#39;t feel like we want to actually set [in stone], this is the schedule for each and every release.
Also, we have to adjust the schedule because of holidays, right? Because you can&#39;t have the code freeze deadline starting on July 4 or in the middle of design or sometime else when we&#39;re going to have a large group of contributors who are out on vacation.
TIM PEPPER: This is something I&#39;ve had to spend some time looking at, thinking about 1.12. Going back to early June as we were tinkering with the code freeze date, starting to think about, well, what are the implications going to be on 1.12? When would these things start falling on the calendar? And then also for 1.11, we had one complexity. If we slipped the release past this week, we start running into the US 4th of July holiday, and we&#39;re not likely to get a lot done.
So much of a slip would mean slipping into the middle of July before we&#39;d really know that we were successfully triaging things. And worst case maybe, we&#39;re quite a bit later into July.
So instead of quarterly with a three-month sort of cadence, well, maybe we&#39;ve accidentally ended up chopping out one month out of the next release or pushing it quite a bit into the end of the year. And that made the deliberation around things quite complex, but thankfully this week, everything&#39;s gone smoothly in the end.

TIM PEPPER: The whole community is thinking about this. There are voices who&#39;d like the cadence to be faster, and there are voices who&#39;d like it to be shorter. And there&#39;s good arguments for both.

JOSH BERKUS: Yeah, certainly. I really honestly think everybody in the world of software recognizes that feature-driven release cycles just don&#39;t work. And a big part of the duties of the release team collectively— several members of the team do this— is yanking things out of the release that are not ready. And the hard part of that is figuring out which things aren&#39;t ready, right? Because the person who&#39;s working on it tends to be super optimistic about what they can get done and what they can get fixed before the deadline.

TIM PEPPER: And this is one of the things I think that&#39;s useful about the process we have in place on the release team for having shadows who spend some time on the release team, working their way up into more of a lead position and gaining some experience, starting to get some exposure to see that optimism and see the processes for vetting.
And it&#39;s even an overstatement to say the process. Just see the way that we build the intuition for how to vet and understand and manage the risk, and really go after and chase things down proactively and early to get resolution in a timely way versus continuing to just all be optimistic and letting things maybe languish and put a release at risk.

JOSH BERKUS: I don&#39;t actually know the history of why we&#39;re not using feature branches. I mean, the reason why we&#39;re not using feature branches pervasively now is that we have to transition from a different system. And I&#39;m not really clear on how we adopted that linear development system. But it&#39;s certainly something we discussed on the release team, because there were issues of features that we thought were going to be ready, and then developed major problems. And we&#39;re like, if we have to back this out, that&#39;s going to be painful. And we did actually have to back one feature out, which involved not pulling out a Git commit, but literally reversing the line changes, which is really not how you want to be doing things.

TIM PEPPER: The other big benefit, I think, to the release branches if they are well integrated with the CI system for continuous integration and testing, you really get the feedback, and you can demonstrate, this set of stuff is ready. And then you can do deferred commitment on the master branch. And what comes in to a particular release on the timely cadence that users are expecting is stuff that&#39;s ready. You don&#39;t have potentially destabilizing things, because you can get a lot more proof and evidence of readiness.

JOSH BERKUS: Well, there&#39;s code review, obviously. So just first of all, process was somebody wants to actually put in a feature, commit, or any kind of merge really, right? So that has to be assigned to one of the SIGs, one of these Special Interest Groups. Possibly more than one, depending on what areas it touches.
And then two generally overlapping groups of people have to approve that. One would be the SIG that it&#39;s assigned to, and the second would be anybody represented in the OWNERS files in the code tree of the directories which get touched.
Now sometimes those are the same group of people. I&#39;d say often, actually. But sometimes they&#39;re not completely the same group of people, because sometimes you&#39;re making a change to the network, but that also happens to touch GCP support and OpenStack support, and so they need to review it as well.
So the first part is the human part, which is a bunch of other people need to look at this. And possibly they&#39;re going to comment &#34;Hey. This is a really weird way to do this. Do you have a reason for it?&#34;
Then the second part of it is the automated testing that happens, the automated acceptance testing that happens via webhook on there. And actually, one of the things that we did that was a significant advancement in this release cycle— and by we, I actually mean not me, but the great folks at [SIG Scalability](https://github.com/kubernetes/community/tree/master/sig-scalability) did— was add an [additional acceptance test](https://k8s-testgrid.appspot.com/sig-release-master-blocking#gce-scale-performance) that does a mini performance test.
Because one of the problems we&#39;ve had historically is our major performance tests are large and take a long time to run, and so by the time we find out that we&#39;re failing the performance tests, we&#39;ve already accumulated, you know, 40, 50 commits. And so now we&#39;re having to do git bisect to find out which of those commits actually caused the performance regression, which can make them very slow to address.
And so adding that performance pre-submit, the performance acceptance test really has helped stabilize performance in terms of new commits. So then we have that level of testing that you have to get past.
And then when we&#39;re done with that level of testing, we run a whole large battery of larger tests— end-to-end tests, performance tests, upgrade and downgrade tests. And one of the things that we&#39;ve added recently and we&#39;re integrating to the process something called conformance tests. And the conformance test is we&#39;re testing whether or not you broke backwards compatibility, because it&#39;s obviously a big deal for Kubernetes users if you do that when you weren&#39;t intending to.
One of the busiest roles in the release team is a role called [CI Signal](https://github.com/kubernetes/sig-release#ci-signal-lead). And it&#39;s that person&#39;s job just to watch all of the tests for new things going red and then to try to figure out why they went red and bring it to people&#39;s attention.

JOSH BERKUS: That goes into release notes. I mean, keep in mind that one of the things that happens with Kubernetes&#39; features is we go through this alpha, beta, general availability phase, right? So a feature&#39;s alpha for a couple of releases and then becomes beta for a release or two, and then it becomes generally available. And part of the idea of having this that may require a feature to go through that cycle for a year or more before its general availability is by the time it&#39;s general availability, we really want it to be, we are not going to change the API for this.
However, stuff happens, and we do occasionally have to do those. And so far, our main way to identify that to people actually is in the release notes. If you look at [the current release notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md#no-really-you-must-do-this-before-you-upgrade), there are actually two things in there right now that are sort of breaking changes.
One of them is the bit with [priority and preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/) in that preemption being on by default now allows badly behaved users of the system to cause trouble in new ways. I&#39;d actually have to look at the release notes to see what the second one was...
TIM PEPPER: The [JSON capitalization case sensitivity](https://github.com/kubernetes/kubernetes/issues/64612).
JOSH BERKUS: Right. Yeah. And that was one of those cases where you have to break backwards compatibility, because due to a library switch, we accidentally enabled people using JSON in a case-insensitive way in certain APIs, which was never supposed to be the case. But because we didn&#39;t have a specific test for that, we didn&#39;t notice that we&#39;d done it.
And so for three releases, people could actually shove in malformed JSON, and Kubernetes would accept it. Well, we have to fix that now. But that does mean that there are going to be users out in the field who have malformed JSON in their configuration management that is now going to break.

JOSH BERKUS: Mm-hmm.
TIM PEPPER: I think that also kind of reminds of one of the other areas— so kind of going back to the question of, well, how do you share word of breaking changes? Well, one of the ways you do that is to have as much quality CI that you can to catch these things that are important. Give the feedback to the developer who&#39;s making the breaking change, such that they don&#39;t make the breaking change. And then you don&#39;t actually have to communicate it out to users.
So some of this is bound to happen, because you always have test escapes. But it&#39;s also a reminder of the need to ensure that you&#39;re also really building and maintaining your test cases and the quality and coverage of your CI system over time.

TIM PEPPER: So I guess it&#39;s a term in the art, but for those who aren&#39;t familiar with it, you have intended behavior that wasn&#39;t covered by test, and as a result, an unintended change happens to that. And instead of your intended behavior being shipped, you&#39;re shipping something else.
JOSH BERKUS: The JSON change is a textbook example of this, which is we were testing that the API would continue to accept correct JSON. We were not testing adequately that it wouldn&#39;t accept incorrect JSON.
TIM PEPPER: A test escape, another way to think of it as you shipped a bug because there was not a test case highlighting the possibility of the bug.

TIM PEPPER: It&#39;s common for us to focus on &#34;I&#39;ve created this feature and I&#39;m testing the positive cases&#34;. And this also comes to thinking about things like secure by default and having a really robust system. A harder piece of engineering often is to think about the failure cases and really actively manage those well.
JOSH BERKUS: I had a conversation with a contributor recently where it became apparent that contributor had never worked on a support team, because their conception of a badly behaved user was, like, a hacker, right? An attacker who comes from outside.
And I&#39;m like, no, no, no. You&#39;re stable of badly behaved users is your own staff. You know, they will do bad things, not necessarily intending to do bad things, but because they&#39;re trying to take a shortcut. And that is actually your primary concern in terms of preventing breaking the system.

JOSH BERKUS: I was on the release team for two cycles, plus I was kind of auditing the release team for half a cycle before that. So in 1.9, I originally joined to be the shadow for bug triage, except I ended up not being the shadow, because the person who was supposed to be the lead for bug triage then dropped out. Then I ended up being the bug triage lead, and had to kind of improvise it because there wasn&#39;t documentation on what was involved in the role at the time.
And then I was [bug triage lead](https://github.com/kubernetes/sig-release/blob/master/README.md#bug-triage-lead) for a second cycle, for the 1.10 cycle, and then took over as release lead for the cycle. And one of the things on my to-do list is to update the requirements to be release lead, because we actually do have written requirements, and to say that the expectation now is that you spend at least two cycles on the release team, one of them either as a lead or as a shadow to the release lead.

JOSH BERKUS: Yeah. Pretty much. There&#39;s more tracking involved than triage. Part of it is just deficiencies in tooling, something we&#39;re looking to address. But things like GitHub API limitations make it challenging to build automated tools that help us intelligently track issues. And we are actually working with GitHub on that. Like, they&#39;ve been helpful. It&#39;s just, they have their own scaling problems.
But then beyond that, you know, a lot of that, it&#39;s what you would expect it to be in terms of what triage says, right? Which is looking at every issue and saying, first of all, is this a real issue? Second, is it a serious issue? Third, who needs to address this?
And that&#39;s a lot of the work, because for anybody who is a regular contributor to Kubernetes, the number of GitHub notifications that they receive per day means that most of us turn our GitHub notifications off.

JOSH BERKUS: Because it&#39;s just this fire hose. And as a result, when somebody really needs to pay attention to something right now, that generally requires a human to go and track them down by email or Slack or whatever they prefer. Twitter in some cases. I&#39;ve done that. And say, hey. We really need you to look at this issue, because it&#39;s about to hold up the beta release.

JOSH BERKUS: Well, we just went through this whole retro, and I put in some recommendations for things. Obviously, some additional automation, which I&#39;m going to be looking at doing now that I&#39;m cycling off of the release team for a quarter and can actually look at more longer term goals, will help, particularly now that we&#39;ve addressed actually some of our GitHub data flow issues.
Beyond that, I put in a whole bunch of recommendations in the retro, but it&#39;s actually up to Tim which recommendations he&#39;s going to try to implement. So I&#39;ll let him [comment].
TIM PEPPER: I think one of the biggest changes that happened in the 1.11 cycle is this emphasis on trying to keep our continuous integration test status always green. That is huge for software development and keeping velocity. If you have this more, I guess at this point antiquated notion of waterfall development, where you do feature development for a while and are accepting of destabilization, and somehow later you&#39;re going to come back and spend a period on stabilization and fixing, that really elongates the feedback loop for developers.
And they don&#39;t realize what was broken, and the problems become much more complex to sort out as time goes by. One, developers aren&#39;t thinking about what it was that they&#39;d been working on anymore. They&#39;ve lost the context to be able to efficiently solve the problem.
But then you start also getting interactions. Maybe a bug was introduced, and other people started working around it or depending on it, and you get complex dependencies then that are harder to fix. And when you&#39;re trying to do that type of complex resolution late in the cycle, it becomes untenable over time. So I think continuing on that and building on it, I&#39;m seeing a little bit more focus on test cases and meaningful test coverage. I think that&#39;s a great cultural change to have happening.
And maybe because I&#39;m following Josh into this role from a bug triage position and in his mentions earlier of just the communications and tracking involved with that versus triage, I do have a bit of a concern that at times, email and Slack are relatively quiet. Some of the SIG meeting notes are a bit sparse or YouTube videos slow to upload. So the general artifacts around choice making I think is an area where we need a little more rigor. So I&#39;m hoping to see some of that.
And that can be just as subtle as commenting on issues like, hey, this commit doesn&#39;t say what it&#39;s doing. And for that reason on the release team, we can&#39;t assess its risk versus value. So could you give a little more information here? Things like that give more information both to the release team and the development community as well, because this is open source. And to collaborate, you really do need to communicate in depth.

JOSH BERKUS: There was a lot of stuff in between.

JOSH BERKUS: You know, believe it or not, there actually are similarities. And here&#39;s where it&#39;s similar, because I was actually thinking about this earlier. So when I was a professional baker, one of the things that I had to do was morning pastry. Like, I was actually in charge of doing several other things for custom orders, but since I had to come to work at 3:00 AM anyway— which also distressingly has similarities with some of this process. Because I had to come to work at 3:00 AM anyway, one of my secondary responsibilities was traying the morning pastry.
And one of the parts of that is you have this great big gas-fired oven with 10 rotating racks in it that are constantly rotating. Like, you get things in and out in the oven by popping them in and out while the racks are moving. That takes a certain amount of skill. You get burn marks on your wrists for your first couple of weeks of work. And then different pastries require a certain number of rotations to be done.
And there&#39;s a lot of similarities to the release cadence, because what you&#39;re doing is you&#39;re popping something in the oven or you&#39;re seeing something get kicked off, and then you have a certain amount of time before you need to check on it or you need to pull it out. And you&#39;re doing that in parallel with a whole bunch of other things. You know, with 40 other trays.

JOSH BERKUS: Yeah. And the other thing is that these deadlines are kind of absolute, right? You can&#39;t say, oh, well, I was reading a magazine article, and I didn&#39;t have time to pull that tray out. It&#39;s too late. The pastry is burned, and you&#39;re going to have to throw it away, and they&#39;re not going to have enough pastry in the front case for the morning rush. And the customers are not interested in your excuses for that.
So from that perspective, from the perspective of saying, hey, we have a bunch of things that need to happen in parallel, they have deadlines and those deadlines are hard deadlines, there it&#39;s actually fairly similar.

TIM PEPPER: I think in some ways I&#39;m more of a traditional journey. I&#39;ve got a computer engineering bachelor&#39;s degree. But I&#39;m also maybe a bit of an outlier. In the late &#39;90s, I found a passion for open source and Linux. Maybe kind of an early adopter, early believer in that.
And was working in the industry in the Bay Area for a while. Got involved in the Silicon Valley and Bay Area Linux users groups a bit, and managed to find work as a Linux sysadmin, and then doing device driver and kernel work and on up into distro. So that was all kind of standard in a way. And then I also did some other work around hardware enablement, high-performance computing, non-uniform memory access. Things that are really, really systems work.
And then about three years ago, my boss was really bending my ear and trying to get me to work on this cloud-related project. And that just felt so abstract and different from the low-level bits type of stuff that I&#39;d been doing.
But kind of grudgingly, I eventually came around to the realization that the cloud is interesting, and it&#39;s so much more complex than local machine-only systems work, the type of things that I&#39;d been doing before. It&#39;s massively distributed and you have a high-latency, low-reliability interconnect on all the nodes in the distributed network. So it&#39;s wildly complex engineering problems that need solved.
And so that got me interested. Started working then on this open source orchestrator for virtual machines and containers. It was written in Go and was having a lot of fun. But it wasn&#39;t Kubernetes, and it was becoming clear that Kubernetes was taking off. So about a year ago, I made the deliberate choice to move over to Kubernetes work.

JOSH BERKUS: The great thing with the release team is that we have this formal mentorship path. And it&#39;s fast, right? That&#39;s the advantage of releasing quarterly, right? Is that within six months, you can go from joining the team as a shadow to being the release lead if you have the time. And you know, by the time you work your way up to release time, you better have support from your boss about this, because you&#39;re going to end up spending a majority of your work time towards the end of the release on release management.
So the answer is to sign up to look when we&#39;re getting into the latter half of release cycle, to sign up as a shadow. Or at the beginning of a release cycle, to sign up as a shadow. Some positions actually can reasonably use more than one shadow. There&#39;s some position that just require a whole ton of legwork like release notes. And as a result, could actually use more than one shadow meaningfully. So there&#39;s probably still places where people could sign up for 1.12. Is that true, Tim?
TIM PEPPER: Definitely. I think— gosh, right now we have 34 volunteers on the release team, which is—

JOSH BERKUS: OK. OK. Maybe not then.
[LAUGH]
TIM PEPPER: It&#39;s potentially becoming a lot of cats to herd. But I think even outside of that formal volunteering to be a named shadow, anybody is welcome to show up to the release team meetings, follow the release team activities on [Slack](http://slack.k8s.io), start understanding how the process works. And really, this is the case all across open source. It doesn&#39;t even have to be the release team. If you&#39;re passionate about networking, start following what SIG Network is doing. It&#39;s the same sort of path, I think, into any area on the project.
Each of the SIGs [has] a channel. So it would be #SIG-whatever the name is. [In our] case, #SIG-Release.
I&#39;d also maybe give a plug for a [talk I did at KubeCon](https://youtu.be/goAph8A20gQ) in Copenhagen this spring, talking about how the release team specifically can be a path for new contributors coming in. And had some ideas and suggestions there for newcomers.


JOSH BERKUS: Two things, I think, really improved things, both for contributors and for the release team. Thing number one was putting a strong emphasis on getting the test grid green well ahead of code freeze.
TIM PEPPER: Definitely.
JOSH BERKUS: Now partly that went well because we had a spectacular CI lead, [Aish Sundar](https://github.com/aishsundar), who&#39;s now in training to become the release lead.
TIM PEPPER: And I&#39;d count that partly as one of the &#34;Where were you lucky?&#34; areas. We happened upon a wonderful person who just popped up and volunteered.
JOSH BERKUS: Yes. And then but part of that was also that we said, hey. You know, we&#39;re not going to do what we&#39;ve done before which is not really care about these tests until code slush. We&#39;re going to care about these tests now.
And importantly— this is really important to the Kubernetes community— when we went to the various SIGs, the SIG Cluster Lifecycle and SIG Scalability and SIG Node and the other ones who were having test failures, and we said this to them. They didn&#39;t say, get lost. I&#39;m busy. They said, what&#39;s failing?

JOSH BERKUS: And so that made a big difference. And the second thing that was pretty much allowed by the first thing was to shorten the code freeze period. Because the code freeze period is frustrating for developers, because if they don&#39;t happen to be working on a 1.11 feature, even if they worked on one before, and they delivered it early in the cycle, and it&#39;s completely done, they&#39;re kind of paralyzed, and they can&#39;t do anything during code freeze. And so it&#39;s very frustrating for them, and we want to make that period as short as possible. And we did that this time, and I think it helped everybody.

JOSH BERKUS: We had a lot of problems with flaky tests. We have a lot of old tests that are not all that well maintained, and they&#39;re testing very complicated things like upgrading a cluster that has 40 nodes. And as a result, these tests have high failure rates that have very little to do with any change in the code.
And so one of the things that happened, and the reason we had a one-day delay in the release is, you know, we&#39;re a week out from release, and just by random luck of the draw, a bunch of these tests all at once got a run of failures. And it turned out that run of failures didn&#39;t actually mean anything, having anything to do with Kubernetes. But there was no way for us to tell that without a lot of research, and we were not going to have enough time for that research without delaying the release.
So one of the things we&#39;re looking to address in the 1.12 cycle is to actually move some of those flaky tests out. Either fix them or move them out of the release blocking category.
TIM PEPPER: In a way, I think this also highlights one of the things that Josh mentioned that went well, the emphasis early on getting the test results green, it allows us to see the extent to which these flakes are such a problem. And then the unlucky occurrence of them all happening to overlap on a failure, again, highlights that these flakes have been called out in the community for quite some time. I mean, at least a year. I know one contributor who was really concerned about them.
But they became a second order concern versus just getting things done in the short term, getting features and proving that the features worked, and kind of accepting in a risk management way on the release team that, yes, those are flakes. We don&#39;t have time to do something about them, and it&#39;s OK. But because of the emphasis on keeping the test always green now, we have the luxury maybe to focus on improving these flakes, and really get to where we have truly high quality CI signal, and can really believe in the results that we have on an ongoing basis.
JOSH BERKUS: And having solved some of the more basic problems, we&#39;re now seeing some of the other problems like coordination between related features. Like we right now have a feature where— and this is one of the sort of backwards compatibility release notes— where the feature went into beta, and is on by default.
And the second feature that was supposed to provide access control for the first feature did not go in as beta, and is not on by default. And the team for the first feature did not realize the second feature was being held up until two days before the release. So it&#39;s going to result in us actually patching something in 11.1.
And so like, we put that into something that didn&#39;t go well. But on the other hand, as Tim points out, a few release cycles ago, we wouldn&#39;t even have identified that as a problem, because we were still struggling with just individual features having a clear idea well ahead of the release of what was going in and what wasn&#39;t going in.
TIM PEPPER: I think something like this also is a case that maybe advocates for the use of feature branches. If these things are related, we might have seen it and done more pre-testing within that branch and pre-integration, and decide maybe to merge a couple of what initially had been disjoint features into a single feature branch, and really convince ourselves that together they were good. And cross all the Ts, dot all the Is on them, and not have something that&#39;s gated on an alpha feature that&#39;s possibly falling away.

JOSH BERKUS: I would say number one where I got lucky is truly having a fantastic team. I mean, we just had a lot of terrific people who were very good and very energetic and very enthusiastic about taking on their release responsibilities including Aish and Tim and Ben and Nick and Misty who took over Docs four weeks into the release. And then went crazy with it and said, well, I&#39;m new here, so I&#39;m going to actually change a bunch of things we&#39;ve been doing that didn&#39;t work in the first place. So that was number one. I mean, that really made honestly all the difference.
And then the second thing, like I said, is that we didn&#39;t have sort of major, unexpected monkey wrenches thrown at us. So in the 1.10 cycle, we actually had two of those, which is why I still count Jace as heroic for pulling off a release that was only a week late.
You know, number one was having the scalability tests start failing for unrelated reasons for a long period, which then masked the fact that they were actually failing for real reasons when we actually got them working again. And as a result, ending up debugging a major and super complicated scalability issue within days of what was supposed to be the original release date. So that was monkey wrench number one for the 1.10 cycle.
Monkey wrench number two for the 1.10 cycle was we got a security hole that needed to be patched. And so again, a week out from what was supposed to be the original release date, we were releasing a security update, and that security update required patching the release branch. And it turns out that patch against the release branch broke a bunch of incoming features. And we didn&#39;t get anything of that magnitude in the 1.11 release, and I&#39;m thankful for that.
TIM PEPPER: Also, I would maybe argue in a way that a portion of that wasn&#39;t just luck. The extent to which this community has a good team, not just the release team but beyond, some of this goes to active work that folks all across the project, but especially in the contributor experience SIG are doing to cultivate a positive and inclusive culture here. And you really see that. When problems crop up, you&#39;re seeing people jump on and really try to constructively tackle them. And it&#39;s really fun to be a part of that.

---




	

	


